# OMSCS ML Math Prep -- Detailed Chapter-Level Reading Plan

This 16-week execution plan specifies exact chapters to read from each
book. If chapter numbering may vary by edition, the section is tagged
with **(unsure)**.

------------------------------------------------------------------------

# PHASE 1 -- CALCULUS FOUNDATION (Weeks 1--4)

Primary Texts: - Stewart -- *Calculus: Early Transcendentals* - OpenStax
-- *Algebra and Trigonometry*

------------------------------------------------------------------------

## Week 1 -- Algebra & Functions

### OpenStax -- Algebra and Trigonometry

-   Ch 1: Prerequisites
-   Ch 2: Equations and Inequalities
-   Ch 3: Functions
-   Ch 4: Polynomial and Rational Functions
-   Ch 5: Exponential and Logarithmic Functions

Focus: - Log rules - Exponent manipulation - Function composition

------------------------------------------------------------------------

## Week 2 -- Single Variable Derivatives

### Stewart -- Calculus

-   Ch 2: Limits and Derivatives
-   Ch 3: Differentiation Rules
-   Ch 4: Applications of Differentiation (optimization sections only)

Key Focus: - Limit definition of derivative - Chain rule - Optimization
structure

------------------------------------------------------------------------

## Week 3 -- Optimization & Integrals (Conceptual)

### Stewart

-   Complete Ch 4 (remaining optimization)
-   Ch 5: Integrals (conceptual understanding only)

Focus: - Critical points - Second derivative test - Convexity

------------------------------------------------------------------------

## Week 4 -- Multivariable Calculus

### Stewart

-   Ch 12: Vectors and Geometry of Space
-   Ch 13: Vector Functions (selected sections)
-   Ch 14: Partial Derivatives
-   Ch 15: Multiple Integrals (overview only)

If numbering differs by edition → **(unsure)**

Focus: - Gradient - Directional derivatives - Jacobian (section may vary
--- unsure)

------------------------------------------------------------------------

# PHASE 2 -- LINEAR ALGEBRA (Weeks 5--8)

Primary Texts: - Deisenroth et al. -- *Mathematics for Machine
Learning* - Strang -- *Introduction to Linear Algebra*

------------------------------------------------------------------------

## Week 5 -- Vectors & Linear Systems

### Mathematics for Machine Learning

-   Ch 2: Linear Algebra (entire chapter)

### Strang

-   Ch 1: Introduction to Vectors
-   Ch 2: Solving Linear Equations

If edition differs → **(unsure)**

Focus: - Span - Linear independence - Systems of equations

------------------------------------------------------------------------

## Week 6 -- Vector Spaces & Orthogonality

### Mathematics for Machine Learning

-   Ch 2 (matrix sections)

### Strang

-   Ch 3: Vector Spaces and Subspaces
-   Ch 4: Orthogonality

Focus: - Rank - Null space - Orthogonal projection

------------------------------------------------------------------------

## Week 7 -- Eigenvalues & Spectral Theory

### Mathematics for Machine Learning

-   Ch 4: Matrix Decompositions

### Strang

-   Ch 5: Eigenvalues and Eigenvectors

Focus: - Diagonalization - Spectral interpretation - Geometric meaning

------------------------------------------------------------------------

## Week 8 -- SVD & PCA

### Mathematics for Machine Learning

-   Ch 4 (SVD section)
-   Ch 10: Dimensionality Reduction (PCA)

### Strang

-   SVD section (often Ch 6 or Ch 7 depending on edition --- unsure)

------------------------------------------------------------------------

# PHASE 3 -- PROBABILITY & STATISTICS (Weeks 9--13)

Primary: - Blitzstein -- *Introduction to Probability* Supplement: -
Downey -- *Think Stats*

------------------------------------------------------------------------

## Week 9 -- Probability Foundations

### Blitzstein

-   Ch 1: Probability and Counting
-   Ch 2: Conditional Probability

Focus: - Law of total probability - Bayes' rule

------------------------------------------------------------------------

## Week 10 -- Random Variables & Expectation

### Blitzstein

-   Ch 3: Random Variables
-   Ch 4: Expectation

Focus: - Linearity of expectation - Variance derivations

------------------------------------------------------------------------

## Week 11 -- Continuous Distributions

### Blitzstein

-   Ch 5: Continuous Random Variables
-   Ch 6: Normal Distribution

If numbering differs → **(unsure)**

Focus: - Gaussian PDF - Multivariate normal (may be appendix --- unsure)

------------------------------------------------------------------------

## Week 12 -- Statistical Inference

### Think Stats

-   Ch 7--9: Estimation and Sampling
-   Ch 10: Regression

(Exact numbering may vary --- unsure)

Focus: - Sampling distributions - Bias vs variance

------------------------------------------------------------------------

## Week 13 -- Statistical Learning Foundations

Primary Texts: - James et al. -- *An Introduction to Statistical
Learning* - Géron -- *Hands-On Machine Learning*

### ISLR

-   Ch 2: Statistical Learning
-   Ch 3: Linear Regression
-   Ch 4: Classification

### Géron

-   Ch 1: The ML Landscape
-   Ch 2: End-to-End ML Project
-   Ch 3: Classification

------------------------------------------------------------------------

# PHASE 4 -- INTEGRATION (Weeks 14--16)

Optional Depth: - Goodfellow et al. -- *Deep Learning*

------------------------------------------------------------------------

## Week 14 -- Linear Regression Derivation

### ISLR

-   Ch 3 (derive equations manually)

### Géron

-   Linear regression sections

------------------------------------------------------------------------

## Week 15 -- Logistic Regression

### ISLR

-   Ch 4 (logistic regression section)

### Géron

-   Logistic regression chapter (often Ch 4 --- unsure)

------------------------------------------------------------------------

## Week 16 -- Neural Networks

### Goodfellow

-   Ch 6: Deep Feedforward Networks
-   Ch 8: Optimization for Training Deep Models

Chapter numbering may vary by edition → **(unsure)**

Focus: - Computational graphs - Backpropagation derivation - Gradient
flow

------------------------------------------------------------------------

# Execution Rule

For each chapter: 1. Extract definitions 2. Re-derive key equations 3.
Translate to ML context 4. Complete 10--15 exercises minimum
