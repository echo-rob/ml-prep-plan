
# OMSCS ML Math Prep – Execution Kit (16 Weeks, Expanded with Resources)

Time Commitment: 6–8 hrs/week  
Goal: Be fully prepared for ML, DL, and NLP coursework.

------------------------------------------------------------
PHASE 1 – CALCULUS FOUNDATION (Weeks 1–4)
------------------------------------------------------------

WEEK 1 – Algebra Fluency

Primary Book:
- Stewart, *Calculus: Early Transcendentals* (Algebra review sections)
- OpenStax, *Algebra and Trigonometry* (Free)

Video:
- Khan Academy – Algebra II
- Khan Academy – Logarithms & Exponentials

Practice:
- 20 algebra manipulation problems
- Rewrite exponential equations in log form
- Plot 5 functions in Python

------------------------------------------------------------

WEEK 2 – Single Variable Derivatives

Primary Book:
- Stewart, *Calculus*, Chapters 2–3
- OpenStax Calculus Volume 1 (Free)

Video:
- 3Blue1Brown – Essence of Calculus (Episodes 1–5)
- Khan Academy – Differential Calculus

Practice:
- Derive 15 functions by hand
- Implement numerical derivative in Python
- Explain derivative intuition in writing

------------------------------------------------------------

WEEK 3 – Optimization

Primary Reading:
- Stewart, Optimization sections
- Géron, *Hands-On ML*, Linear Regression chapter (optimization intuition)

Video:
- StatQuest – Gradient Descent
- 3Blue1Brown – Gradient Descent visualizations

Practice:
- Minimize quadratic functions analytically
- Implement 1D gradient descent
- Compare learning rates

------------------------------------------------------------

WEEK 4 – Multivariable Calculus

Primary Book:
- Stewart, Multivariable chapters
- OpenStax Calculus Volume 3 (Partial Derivatives)

Video:
- Khan Academy – Multivariable Calculus
- MIT OCW 18.02 – Selected lectures

Practice:
- Compute partial derivatives manually
- Compute gradient vectors
- Plot contour maps in Python

AUDIT 1:
Explain gradient descent mathematically without notes.

------------------------------------------------------------
PHASE 2 – LINEAR ALGEBRA (Weeks 5–8)
------------------------------------------------------------

Primary Book (Core Text):
- Deisenroth et al., *Mathematics for Machine Learning*
- Gilbert Strang, *Introduction to Linear Algebra*

Video:
- MIT OCW 18.06 – Gilbert Strang lectures
- 3Blue1Brown – Essence of Linear Algebra series

------------------------------------------------------------

WEEK 5 – Vectors

Reading:
- MML Chapter: Vector Spaces
- Strang Chapters 1–2

Practice:
- Compute dot products manually
- Implement cosine similarity
- Explain L1 vs L2 norms

------------------------------------------------------------

WEEK 6 – Matrices

Reading:
- Strang Chapters 2–3
- MML Linear Transformations section

Practice:
- Multiply matrices by hand
- Implement matrix multiplication
- Explain rank geometrically

------------------------------------------------------------

WEEK 7 – Eigenvalues

Reading:
- Strang Chapter 6
- MML Eigenvalue section

Video:
- 3Blue1Brown – Eigenvectors explained

Practice:
- Solve eigenvalues for 2x2 matrices
- Perform PCA in Python

------------------------------------------------------------

WEEK 8 – SVD

Reading:
- MML SVD section
- Strang SVD chapter

Practice:
- Perform SVD in NumPy
- Apply PCA to dataset

AUDIT 2:
Explain PCA and eigenvalues geometrically without notes.

------------------------------------------------------------
PHASE 3 – PROBABILITY & STATISTICS (Weeks 9–13)
------------------------------------------------------------

Primary Book:
- Deisenroth et al., Probability chapters
- Blitzstein, *Introduction to Probability* (Harvard STAT110)

Supplement:
- Downey, *Think Stats* (Applied Python-based stats)

Video:
- Harvard STAT110 lectures
- StatQuest – Probability & Statistics playlist

------------------------------------------------------------

WEEK 9 – Random Variables

Reading:
- Blitzstein Chapters 2–3
- MML Probability basics

Practice:
- Simulate distributions in Python
- Compute expectation and variance manually

------------------------------------------------------------

WEEK 10 – Conditional Probability

Reading:
- Blitzstein Bayes sections
- MML Bayes Rule section

Practice:
- Apply Bayes theorem to 5 examples
- Implement Naive Bayes

------------------------------------------------------------

WEEK 11 – Gaussian Distributions

Reading:
- MML Gaussian section
- Blitzstein Normal Distribution chapter

Practice:
- Plot normal distribution
- Visualize 2D Gaussian

------------------------------------------------------------

WEEK 12 – Maximum Likelihood

Reading:
- MML MLE section
- Géron Logistic Regression chapter

Practice:
- Derive MLE for mean
- Connect likelihood to logistic regression

------------------------------------------------------------

WEEK 13 – Bias vs Variance

Reading:
- Géron Model Evaluation chapter
- ISLR (James et al.) Chapters 2–5

Practice:
- Perform cross-validation
- Interpret confusion matrix

AUDIT 3:
Explain bias-variance tradeoff clearly without notes.

------------------------------------------------------------
PHASE 4 – INTEGRATION (Weeks 14–16)
------------------------------------------------------------

Primary Book:
- Géron, *Hands-On Machine Learning*

------------------------------------------------------------

WEEK 14 – Linear Regression (From Scratch)

Reading:
- Géron Linear Models chapter

Practice:
- Implement linear regression manually
- Add L2 regularization

------------------------------------------------------------

WEEK 15 – Logistic Regression

Reading:
- Géron Logistic Regression chapter

Practice:
- Implement sigmoid
- Compute cross-entropy loss
- Train classifier from scratch

------------------------------------------------------------

WEEK 16 – Neural Networks

Reading:
- Géron Neural Networks chapter
- Goodfellow et al., *Deep Learning*, Chapter 6 (Optional depth)

Video:
- 3Blue1Brown – Backpropagation visualizations
- StatQuest – Neural Networks explained

Practice:
- Build 1-hidden-layer neural network
- Explain backpropagation conceptually

------------------------------------------------------------
FINAL AUDIT:
Explain backprop, gradient descent, embeddings, and evaluation metrics without notes.
