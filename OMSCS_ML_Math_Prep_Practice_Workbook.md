
# OMSCS ML Math Prep – Dedicated Practice Problem Workbook

This workbook contains additional structured practice problems
for each phase of the 16-week ML math preparation plan.

------------------------------------------------------------
PHASE 1 – CALCULUS FOUNDATION
------------------------------------------------------------

WEEK 1 – Algebra Fluency

1. Solve for x: 5e^(3x) = 40  
2. Solve for x: ln(2x + 1) = 3  
3. Rearrange: y = (ax + b)/(cx + d) for x  
4. Simplify: (x^2 - 9)/(x - 3)  
5. Convert between log bases using change-of-base formula  
6. Graph f(x)=e^(-x^2) and describe shape  
7. Solve quadratic using completing the square  
8. Explain difference between exponential growth and polynomial growth  

------------------------------------------------------------

WEEK 2 – Single Variable Derivatives

1. Derive: 4x^4 − 3x^2 + 7  
2. Derive: ln(x^2 + 1)  
3. Derive: e^(2x^2)  
4. Use product rule on (x^3)(sin x)  
5. Use chain rule on (5x^3 − 1)^6  
6. Find critical points of x^3 − 6x^2 + 9x  
7. Explain second derivative test  
8. Numerically approximate derivative of sin(x) at x=1  

------------------------------------------------------------

WEEK 3 – Optimization

1. Minimize f(x)=3x^2 + 12x + 5 analytically  
2. Implement gradient descent for f(x)=x^4 − 3x^3  
3. Experiment with 3 learning rates  
4. Explain convex vs non-convex functions  
5. Why can gradient descent get stuck in local minima?  
6. What happens if learning rate is too large?  

------------------------------------------------------------

WEEK 4 – Multivariable Calculus

1. Compute ∂f/∂x and ∂f/∂y for f(x,y)=x^2y + y^3  
2. Compute gradient of f(x,y)=x^2 + y^2  
3. Find stationary point of f(x,y)=x^2 + y^2 − 4x − 6y  
4. Explain meaning of gradient magnitude  
5. Implement 2D gradient descent  

------------------------------------------------------------
PHASE 2 – LINEAR ALGEBRA
------------------------------------------------------------

WEEK 5 – Vectors

1. Compute dot product of [1,2,3] and [4,5,6]  
2. Compute cosine similarity between two vectors  
3. Compute L1 and L2 norms of [3, -4]  
4. Explain geometric meaning of orthogonality  
5. Show when cosine similarity equals 1  

------------------------------------------------------------

WEEK 6 – Matrices

1. Multiply two 2x2 matrices manually  
2. Multiply 3x2 and 2x3 matrices  
3. Determine rank of small matrices  
4. Explain when matrix is invertible  
5. Compute inverse of 2x2 matrix  

------------------------------------------------------------

WEEK 7 – Eigenvalues

1. Find eigenvalues of [[3,1],[0,2]]  
2. Find eigenvectors  
3. Explain why eigenvectors represent principal directions  
4. Compute covariance matrix for small dataset  
5. Perform PCA manually on 2D dataset  

------------------------------------------------------------

WEEK 8 – SVD

1. Perform SVD on small 2x2 matrix  
2. Reconstruct matrix using top singular value  
3. Explain low-rank approximation  
4. Compare PCA vs SVD  
5. Why does dimensionality reduction lose information?  

------------------------------------------------------------
PHASE 3 – PROBABILITY & STATISTICS
------------------------------------------------------------

WEEK 9 – Random Variables

1. Compute expectation of discrete variable  
2. Compute variance of given distribution  
3. Simulate coin flips and estimate probability  
4. Compare theoretical vs empirical mean  
5. Explain law of large numbers  

------------------------------------------------------------

WEEK 10 – Conditional Probability

1. Apply Bayes rule to medical test example  
2. Compute posterior probability given prior  
3. Show independence example  
4. Build Naive Bayes classifier for small dataset  
5. Interpret false positive rate mathematically  

------------------------------------------------------------

WEEK 11 – Gaussian Distributions

1. Compute probability under normal curve  
2. Standardize variable using z-score  
3. Plot Gaussian distribution  
4. Compute covariance of 2D dataset  
5. Explain multivariate Gaussian intuition  

------------------------------------------------------------

WEEK 12 – Maximum Likelihood

1. Derive MLE for mean of normal distribution  
2. Derive MLE for variance  
3. Explain log-likelihood vs likelihood  
4. Connect MLE to linear regression  
5. Explain logistic regression loss function  

------------------------------------------------------------

WEEK 13 – Bias vs Variance

1. Simulate overfitting example  
2. Perform k-fold cross-validation  
3. Compute precision, recall, F1-score  
4. Explain bias-variance tradeoff graphically  
5. Why does regularization reduce variance?  

------------------------------------------------------------
PHASE 4 – INTEGRATION
------------------------------------------------------------

WEEK 14 – Linear Regression

1. Implement linear regression from scratch  
2. Add L2 regularization  
3. Compare MSE before/after regularization  
4. Plot loss over epochs  

------------------------------------------------------------

WEEK 15 – Logistic Regression

1. Implement sigmoid function  
2. Compute cross-entropy loss  
3. Train classifier using gradient descent  
4. Evaluate confusion matrix  

------------------------------------------------------------

WEEK 16 – Neural Networks

1. Build 1-hidden-layer network  
2. Implement forward pass  
3. Derive backpropagation steps  
4. Add L2 regularization  
5. Compare train vs validation loss  

------------------------------------------------------------

End of Workbook
